# Joint-Representation-Learning
Learning joint representations of multimodal embeddings using contrastive learning. 
This Jupyter notebook shows how CLIP model is used for creating image labels using contrastive learning. The image labels were used from CIFAR-100 dataset. The labels are then input to different large language models (LLMs) to create a description of the image. 
# Using LLMs
For using GPT-3.5 model, you must have OpenAI API key. For using mixtral-8x7B-Instruct-v0_1 model, you can create a free API from https://www.clarifai.com/blog/run-mistral-7b-with-an-api
